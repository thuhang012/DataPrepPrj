{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31959160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the project root \n",
    "project_root = Path.cwd().parent\n",
    "\n",
    "# Add project root to sys.path\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b2df709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import importlib\n",
    "\n",
    "# Import and reload to get latest changes\n",
    "import src.preprocessing as preprocessing\n",
    "importlib.reload(preprocessing)\n",
    "\n",
    "from src.preprocessing import (\n",
    "    load_raw_2015,\n",
    "    build_and_save_preprocessed,\n",
    "    classify_variables,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7fc78808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data shape: (441456, 171)\n",
      "Columns: 171\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GENHLTH</th>\n",
       "      <th>PHYSHLTH</th>\n",
       "      <th>MENTHLTH</th>\n",
       "      <th>POORHLTH</th>\n",
       "      <th>HLTHPLN1</th>\n",
       "      <th>PERSDOC2</th>\n",
       "      <th>MEDCOST</th>\n",
       "      <th>CHECKUP1</th>\n",
       "      <th>BPHIGH4</th>\n",
       "      <th>BPMEDS</th>\n",
       "      <th>...</th>\n",
       "      <th>_PAREC1</th>\n",
       "      <th>_PASTAE1</th>\n",
       "      <th>_LMTACT1</th>\n",
       "      <th>_LMTWRK1</th>\n",
       "      <th>_LMTSCL1</th>\n",
       "      <th>_RFSEAT2</th>\n",
       "      <th>_RFSEAT3</th>\n",
       "      <th>_FLSHOT6</th>\n",
       "      <th>_PNEUMO2</th>\n",
       "      <th>_AIDTST3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>18</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>88</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>88</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 171 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   GENHLTH  PHYSHLTH  MENTHLTH  POORHLTH  HLTHPLN1  PERSDOC2  MEDCOST  \\\n",
       "0      5.0      15.0        18      10.0         1         1      2.0   \n",
       "1      3.0      88.0        88       NaN         2         1      1.0   \n",
       "2      4.0      15.0        88      88.0         1         2      2.0   \n",
       "3      5.0      30.0        30      30.0         1         2      1.0   \n",
       "4      5.0      20.0        88      30.0         1         1      2.0   \n",
       "\n",
       "   CHECKUP1  BPHIGH4  BPMEDS  ...  _PAREC1  _PASTAE1  _LMTACT1  _LMTWRK1  \\\n",
       "0       1.0      1.0     1.0  ...        4         2       1.0       1.0   \n",
       "1       4.0      3.0     NaN  ...        2         2       3.0       3.0   \n",
       "2       1.0      3.0     NaN  ...        9         9       9.0       9.0   \n",
       "3       1.0      1.0     1.0  ...        4         2       1.0       1.0   \n",
       "4       1.0      3.0     NaN  ...        4         2       1.0       1.0   \n",
       "\n",
       "   _LMTSCL1  _RFSEAT2  _RFSEAT3  _FLSHOT6  _PNEUMO2  _AIDTST3  \n",
       "0       1.0         1         1       NaN       NaN       1.0  \n",
       "1       4.0         2         2       NaN       NaN       2.0  \n",
       "2       9.0         9         9       9.0       9.0       NaN  \n",
       "3       1.0         1         1       NaN       NaN       9.0  \n",
       "4       1.0         1         1       NaN       NaN       1.0  \n",
       "\n",
       "[5 rows x 171 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load raw data\n",
    "df_raw = load_raw_2015()\n",
    "print(f\"Raw data shape: {df_raw.shape}\")\n",
    "print(f\"Columns: {df_raw.shape[1]}\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef10023",
   "metadata": {},
   "source": [
    "## Step 2: Apply Codebook Cleaning\n",
    "\n",
    "BRFSS uses coded values for missing data (e.g., 7, 9, 77, 99, 777, 999). We need to convert these to proper NaN values so they're recognized as missing data in our analysis.\n",
    "\n",
    "**Examples:**\n",
    "- Yes/No questions: 1=Yes, 2=No, 7=Don't know, 9=Refused → Convert 7,9 to NaN\n",
    "- Frequency questions: 555=Never, 777=Don't know, 999=Refused → Convert 777,999 to NaN, 555 to 0\n",
    "- Days (0-30): 88=None, 77=Don't know, 99=Refused → Convert 88 to 0, 77,99 to NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7850eb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After codebook cleaning: (441456, 171)\n",
      "\n",
      "Missing values before: 9,167,996\n",
      "Missing values after: 11,144,015\n",
      "Increase: 1,976,019\n",
      "Increase: 1,976,019\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing import apply_codebook_cleaning\n",
    "\n",
    "df_cleaned = apply_codebook_cleaning(df_raw)\n",
    "print(f\"After codebook cleaning: {df_cleaned.shape}\")\n",
    "print(f\"\\nMissing values before: {df_raw.isna().sum().sum():,}\")\n",
    "print(f\"Missing values after: {df_cleaned.isna().sum().sum():,}\")\n",
    "print(f\"Increase: {df_cleaned.isna().sum().sum() - df_raw.isna().sum().sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461ffb65",
   "metadata": {},
   "source": [
    "## Step 3: Drop Raw Columns When Calculated Versions Exist\n",
    "\n",
    "BRFSS provides both raw and calculated versions of some variables. For example:\n",
    "- `WEIGHT2` (raw weight in pounds) → `WTKG3` (calculated weight in kg)\n",
    "- `HEIGHT3` (raw height) → `HTIN4`, `HTM4`, `_BMI5` (calculated height and BMI)\n",
    "- `ALCDAY5` (raw alcohol frequency) → `_DRNKWEK` (calculated drinks per week)\n",
    "\n",
    "**Why drop raw versions?**\n",
    "- Avoid redundancy and multicollinearity\n",
    "- Calculated versions are standardized and easier to use\n",
    "- Reduces dimensionality\n",
    "\n",
    "Let's check which columns will be dropped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8989ce25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw columns that will be dropped if calculated versions exist:\n",
      "\n",
      "✓ WEIGHT2 → ['WTKG3', '_BMI5']\n",
      "✓ HEIGHT3 → ['HTIN4', 'HTM4', '_BMI5']\n",
      "✓ ALCDAY5 → ['DROCDY3_', '_DRNKWEK', '_RFDRHV5', '_RFBING5']\n",
      "✓ FRUITJU1 → ['FTJUDA1_']\n",
      "✓ FRUIT1 → ['FRUTDA1_']\n",
      "✓ FVBEANS → ['BEANDAY_']\n",
      "✓ FVGREEN → ['GRENDAY_']\n",
      "✓ FVORANG → ['ORNGDAY_']\n",
      "✓ VEGETAB1 → ['VEGEDA1_']\n",
      "✓ EXEROFT1 → ['PAFREQ1_']\n",
      "✓ EXERHMM1 → ['PADUR1_']\n",
      "✓ EXEROFT2 → ['PAFREQ2_']\n",
      "✓ EXERHMM2 → ['PADUR2_']\n",
      "✓ STRENGTH → ['STRFREQ_', '_PASTRNG']\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing import RAW_TO_CALC, drop_raw_when_calc_exists\n",
    "\n",
    "# Check which columns will be dropped\n",
    "print(\"Raw columns that will be dropped if calculated versions exist:\\n\")\n",
    "for raw_col, calc_cols in RAW_TO_CALC.items():\n",
    "    if raw_col in df_cleaned.columns:\n",
    "        existing_calc = [c for c in calc_cols if c in df_cleaned.columns]\n",
    "        if existing_calc:\n",
    "            print(f\"✓ {raw_col} → {existing_calc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0efc7b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before: 171 columns\n",
      "After: 157 columns\n",
      "Dropped: 14 columns\n"
     ]
    }
   ],
   "source": [
    "# Apply the dropping\n",
    "df_no_raw = drop_raw_when_calc_exists(df_cleaned)\n",
    "print(f\"\\nBefore: {df_cleaned.shape[1]} columns\")\n",
    "print(f\"After: {df_no_raw.shape[1]} columns\")\n",
    "print(f\"Dropped: {df_cleaned.shape[1] - df_no_raw.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183c1053",
   "metadata": {},
   "source": [
    "## Step 4: Identify and Drop High-Missing Columns\n",
    "\n",
    "Columns with >50% missing values provide little information and can harm model performance. Let's identify them first before deciding to drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cadc1d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with >50% missing values (17):\n",
      "\n",
      "  _CHISPNC: 86.0% missing\n",
      "  JOINPAIN: 70.3% missing\n",
      "  ARTHDIS2: 70.0% missing\n",
      "  _PNEUMO2: 69.5% missing\n",
      "  ARTHSOCL: 69.4% missing\n",
      "  LMTJOIN3: 69.4% missing\n",
      "  _FLSHOT6: 68.5% missing\n",
      "  TRNSGNDR: 62.8% missing\n",
      "  BPMEDS: 59.7% missing\n",
      "  FLSHTMY2: 59.5% missing\n",
      "  SMOKDAY2: 58.4% missing\n",
      "  IMFVPLAC: 56.8% missing\n",
      "  PADUR2_: 56.4% missing\n",
      "  PAFREQ2_: 55.9% missing\n",
      "  MAXDRNKS: 53.9% missing\n",
      "  AVEDRNK2: 53.1% missing\n",
      "  DRNK3GE5: 53.0% missing\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing import get_high_missing_columns, drop_columns\n",
    "\n",
    "# Identify high-missing columns\n",
    "high_missing_cols = get_high_missing_columns(df_no_raw, threshold=0.5)\n",
    "\n",
    "print(f\"Columns with >50% missing values ({len(high_missing_cols)}):\\n\")\n",
    "missing_ratio = df_no_raw[high_missing_cols].isna().mean().sort_values(ascending=False)\n",
    "for col, ratio in missing_ratio.items():\n",
    "    print(f\"  {col}: {ratio*100:.1f}% missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34416ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 17 columns:\n",
      "  - _CHISPNC\n",
      "  - JOINPAIN\n",
      "  - ARTHDIS2\n",
      "  - _PNEUMO2\n",
      "  - ARTHSOCL\n",
      "  - LMTJOIN3\n",
      "  - _FLSHOT6\n",
      "  - TRNSGNDR\n",
      "  - BPMEDS\n",
      "  - FLSHTMY2\n",
      "  - SMOKDAY2\n",
      "  - IMFVPLAC\n",
      "  - PADUR2_\n",
      "  - PAFREQ2_\n",
      "  - MAXDRNKS\n",
      "  - AVEDRNK2\n",
      "  - DRNK3GE5\n",
      "\n",
      "✓ Kept 140 columns\n",
      "\n",
      "✓ Kept 140 columns\n"
     ]
    }
   ],
   "source": [
    "# Drop high-missing columns\n",
    "df_no_high_missing = drop_columns(df_no_raw, high_missing_cols, verbose=True)\n",
    "print(f\"\\n✓ Kept {df_no_high_missing.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d8fb14",
   "metadata": {},
   "source": [
    "## Step 5: Drop Questionnaire Metadata Columns\n",
    "\n",
    "Columns like questionnaire version (`QSTVER`) and interview language (`QSTLANG`) are administrative metadata, not health data. They don't contribute to predicting health outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ffcf1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dropping 2 questionnaire metadata columns: ['QSTVER', 'QSTLANG']\n",
      "Columns remaining: 138\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing import drop_questionnaire_metadata_columns\n",
    "\n",
    "df_no_metadata = drop_questionnaire_metadata_columns(df_no_high_missing)\n",
    "print(f\"Columns remaining: {df_no_metadata.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bbcc2a",
   "metadata": {},
   "source": [
    "## Step 6: Drop High-Cardinality Text Columns\n",
    "\n",
    "Free-text columns with many unique values (like \"other activity type\") are difficult to use in models and typically provide little predictive value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e85ba850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dropping 2 high-cardinality text columns (>50 unique values):\n",
      "  - EXACTOT1 (5642 unique values)\n",
      "  - EXACTOT2 (7623 unique values)\n",
      "Columns remaining: 136\n",
      "Columns remaining: 136\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing import drop_high_cardinality_text_columns\n",
    "\n",
    "df_no_text = drop_high_cardinality_text_columns(df_no_metadata, threshold=50)\n",
    "print(f\"Columns remaining: {df_no_text.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c109de1",
   "metadata": {},
   "source": [
    "## Step 7: Fix Boolean Encoding\n",
    "\n",
    "BRFSS encodes Yes/No questions as 1=Yes, 2=No. For modeling, we need 0=No, 1=Yes (standard binary encoding).\n",
    "\n",
    "Also, some columns have scientific notation issues (5.4e-79 instead of 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4b022b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before encoding fix (sample boolean columns):\n",
      "  HLTHPLN1: [np.float64(1.0), np.float64(2.0)]\n",
      "  MEDCOST: [np.float64(1.0), np.float64(2.0)]\n",
      "  BLOODCHO: [np.float64(1.0), np.float64(2.0)]\n",
      "  HLTHPLN1: [np.float64(1.0), np.float64(2.0)]\n",
      "  MEDCOST: [np.float64(1.0), np.float64(2.0)]\n",
      "  BLOODCHO: [np.float64(1.0), np.float64(2.0)]\n",
      "\n",
      "Mapping 1->0, 2->1 for 55 boolean columns\n",
      "\n",
      "Mapping 1->0, 2->1 for 55 boolean columns\n",
      "✓ Fixed encoding for 60 boolean columns\n",
      "\n",
      "After encoding fix:\n",
      "  HLTHPLN1: [np.float64(0.0), np.float64(1.0)]\n",
      "  MEDCOST: [np.float64(0.0), np.float64(1.0)]\n",
      "  BLOODCHO: [np.float64(0.0), np.float64(1.0)]\n",
      "✓ Fixed encoding for 60 boolean columns\n",
      "\n",
      "After encoding fix:\n",
      "  HLTHPLN1: [np.float64(0.0), np.float64(1.0)]\n",
      "  MEDCOST: [np.float64(0.0), np.float64(1.0)]\n",
      "  BLOODCHO: [np.float64(0.0), np.float64(1.0)]\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing import fix_boolean_encoding\n",
    "\n",
    "# Check before\n",
    "print(\"Before encoding fix (sample boolean columns):\")\n",
    "bool_sample = [col for col in df_no_text.columns if df_no_text[col].dropna().nunique() == 2][:3]\n",
    "for col in bool_sample:\n",
    "    print(f\"  {col}: {sorted(df_no_text[col].dropna().unique())}\")\n",
    "\n",
    "df_fixed = fix_boolean_encoding(df_no_text)\n",
    "\n",
    "# Check after\n",
    "print(\"\\nAfter encoding fix:\")\n",
    "for col in bool_sample:\n",
    "    print(f\"  {col}: {sorted(df_fixed[col].dropna().unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489f082",
   "metadata": {},
   "source": [
    "## Step 8: Drop Rows with Missing Target Variable\n",
    "\n",
    "For supervised learning, we need the target variable (`_MICHD` - cardiovascular disease indicator). Rows without this value cannot be used for training or evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "29fc1605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping missing target:\n",
      "  Total rows: 441,456\n",
      "  Rows with missing target: 3,942\n",
      "  Percentage missing: 0.89%\n",
      "\n",
      "After dropping missing target:\n",
      "  Total rows: 437,514\n",
      "  Rows dropped: 3,942\n"
     ]
    }
   ],
   "source": [
    "# Define target variable\n",
    "TARGET_COL = '_MICHD'\n",
    "\n",
    "# Check missing target values\n",
    "print(f\"Before dropping missing target:\")\n",
    "print(f\"  Total rows: {df_fixed.shape[0]:,}\")\n",
    "print(f\"  Rows with missing target: {df_fixed[TARGET_COL].isna().sum():,}\")\n",
    "print(f\"  Percentage missing: {df_fixed[TARGET_COL].isna().mean()*100:.2f}%\")\n",
    "\n",
    "# Drop rows with missing target\n",
    "df_final = df_fixed.dropna(subset=[TARGET_COL])\n",
    "\n",
    "print(f\"\\nAfter dropping missing target:\")\n",
    "print(f\"  Total rows: {df_final.shape[0]:,}\")\n",
    "print(f\"  Rows dropped: {df_fixed.shape[0] - df_final.shape[0]:,}\")\n",
    "\n",
    "# Update df_pre\n",
    "df_pre = df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fee674",
   "metadata": {},
   "source": [
    "## Step 9: Classify Variables and Save Final Dataset\n",
    "\n",
    "Now classify columns into numeric, categorical, and boolean types, then save the final preprocessed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8923d9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Variable Classification ===\n",
      "Numeric: 42\n",
      "Categorical: 34\n",
      "Boolean: 60\n",
      "Saved cleaned dataset to: D:\\dataprep_final\\DataPrepPrj\\data\\processed\\clean_2015.csv\n",
      "\n",
      "✓ Final preprocessed dataset: (437514, 136)\n",
      "✓ Saved to: D:\\dataprep_final\\DataPrepPrj\\data\\processed\\clean_2015.csv\n",
      "Saved cleaned dataset to: D:\\dataprep_final\\DataPrepPrj\\data\\processed\\clean_2015.csv\n",
      "\n",
      "✓ Final preprocessed dataset: (437514, 136)\n",
      "✓ Saved to: D:\\dataprep_final\\DataPrepPrj\\data\\processed\\clean_2015.csv\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing import save_preprocessed_csv\n",
    "\n",
    "# Classify variables\n",
    "numeric_cols, categorical_cols, boolean_cols = classify_variables(df_pre)\n",
    "\n",
    "# Save\n",
    "save_path = save_preprocessed_csv(df_pre, \"clean_2015.csv\")\n",
    "\n",
    "print(f\"\\n✓ Final preprocessed dataset: {df_pre.shape}\")\n",
    "print(f\"✓ Saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1d35ddb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FINAL PREPROCESSED DATASET SUMMARY\n",
      "==================================================\n",
      "Total columns: 136\n",
      "Total rows: 437514\n",
      "\n",
      "Variable breakdown:\n",
      "  - Numeric: 42\n",
      "  - Categorical: 34\n",
      "  - Boolean: 60\n",
      "\n",
      "Missing values: 5,177,882 (8.70%)\n"
     ]
    }
   ],
   "source": [
    "# Summary of final preprocessed data\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"FINAL PREPROCESSED DATASET SUMMARY\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total columns: {df_pre.shape[1]}\")\n",
    "print(f\"Total rows: {df_pre.shape[0]}\")\n",
    "print(f\"\\nVariable breakdown:\")\n",
    "print(f\"  - Numeric: {len(numeric_cols)}\")\n",
    "print(f\"  - Categorical: {len(categorical_cols)}\")\n",
    "print(f\"  - Boolean: {len(boolean_cols)}\")\n",
    "print(f\"\\nMissing values: {df_pre.isna().sum().sum():,} ({df_pre.isna().sum().sum() / df_pre.size * 100:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataprep_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
